# scripts/fetch_gold.py
# -----------------------------------------------------------------------------
# M·ª§C TI√äU
# 1) L·∫•y d·ªØ li·ªáu v√†ng t·ª´ Google Sheets b·∫±ng c√°ch t·∫£i file XLSX export (public link)
# 2) Xu·∫•t ra 2 file trong repo:
#    - data/gold_live_new.csv        : d·ªØ li·ªáu d·∫°ng b·∫£ng (sheet GOLD_PRICE)
#    - data/gold_live_raw_new.log    : log raw JSON (sheet RAW_DATA), 1 d√≤ng / snapshot
#
# L∆ØU √ù QUAN TR·ªåNG
# - Google Sheet ph·∫£i b·∫≠t "Anyone with the link can view" (ai c√≥ link ƒë·ªÅu xem ƒë∆∞·ª£c)
# - Kh√¥ng d√πng Google API/OAuth, ch·ªâ t·∫£i XLSX public.
# - RAW log s·∫Ω dedup theo TIMESTAMP ƒë·ªÉ log kh√¥ng b·ªã ph√¨nh.
# - TABLE csv s·∫Ω dedup theo (Ng√†y | Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi | M√£ v√†ng)
# -----------------------------------------------------------------------------

import os
import re
import pandas as pd

# ========== CONFIG: Google Sheet ==========
SHEET_ID = "12IidFzGCo4yzUN77SqUTiUsF4qLp7RtAMSUR35IhCKs"

# Export XLSX tr·ª±c ti·∫øp t·ª´ Google Sheets (y√™u c·∫ßu sheet public view)
INPUT_XLSX_URL = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/export?format=xlsx"

# ========== OUTPUT FILES ==========
OUT_TABLE_NEW = "data/gold_live_new.csv"       # b·∫£ng d·ªØ li·ªáu
OUT_RAW_LOG = "data/gold_live_raw_new.log"     # raw JSON log (1 d√≤ng / snapshot)

# ========== SHEET NAMES ==========
SHEET_PRICE = "GOLD_PRICE"   # sheet ch·ª©a b·∫£ng gi√° d·∫°ng table
SHEET_RAW = "RAW_DATA"       # sheet ch·ª©a raw json log

# ========== HEADERS EXPECTED IN GOLD_PRICE ==========
# Script s·∫Ω ƒë·∫£m b·∫£o ƒë·ªß c√°c c·ªôt n√†y, thi·∫øu c·ªôt n√†o s·∫Ω t·∫°o c·ªôt r·ªóng.
HEADERS_TABLE = [
    "Ng√†y",
    "Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi",
    "Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t d·ªØ li·ªáu",
    "M√£ v√†ng",
    "Lo·∫°i v√†ng",
    "Gi√° mua",
    "Gi√° b√°n",
    "Day change buy",
    "Day change sell",
    "Currency",
    "S·ªë l·∫ßn update",
]


def fetch_from_sheets(xlsx_url: str) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    ƒê·ªçc 2 sheet t·ª´ file XLSX export c·ªßa Google Sheets:
      - GOLD_PRICE: d·ªØ li·ªáu d·∫°ng b·∫£ng
      - RAW_DATA  : d·ªØ li·ªáu raw (√©p string ƒë·ªÉ tr√°nh pandas t·ª± parse)
    """
    try:
        # GOLD_PRICE: ƒë·ªçc b√¨nh th∆∞·ªùng
        df_price = pd.read_excel(xlsx_url, sheet_name=SHEET_PRICE)

        # RAW_DATA: √©p dtype=str ƒë·ªÉ JSON/timestamp kh√¥ng b·ªã pandas bi·∫øn ƒë·ªïi
        df_raw = pd.read_excel(xlsx_url, sheet_name=SHEET_RAW, dtype=str)

        return df_price, df_raw

    except Exception as e:
        # Th∆∞·ªùng l·ªói do:
        # - Sheet ch∆∞a public
        # - Sai t√™n sheet
        # - Link b·ªã ch·∫∑n/qu√° quy·ªÅn
        raise RuntimeError(
            "Kh√¥ng th·ªÉ ƒë·ªçc Google Sheet. H√£y ƒë·∫£m b·∫£o sheet ƒë√£ b·∫≠t "
            "'B·∫•t k·ª≥ ai c√≥ li√™n k·∫øt ƒë·ªÅu c√≥ th·ªÉ xem'. "
            f"L·ªói: {e}"
        )


def _ensure_headers_table(df: pd.DataFrame) -> pd.DataFrame:
    """
    ƒê·∫£m b·∫£o df c√≥ ƒë·∫ßy ƒë·ªß c·ªôt theo HEADERS_TABLE v√† ƒë√∫ng th·ª© t·ª±.
    N·∫øu thi·∫øu c·ªôt -> t·∫°o c·ªôt r·ªóng.
    """
    df = df.copy()
    for col in HEADERS_TABLE:
        if col not in df.columns:
            df[col] = ""
    # Tr·∫£ v·ªÅ ƒë√∫ng th·ª© t·ª± c·ªôt ƒë·ªÉ output consistent
    return df[HEADERS_TABLE]


def save_dedup_table(df_new: pd.DataFrame) -> None:
    """
    L∆∞u sheet GOLD_PRICE ra file OUT_TABLE_NEW (CSV) theo h∆∞·ªõng "append + dedup".

    Dedup key = (Ng√†y | Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi | M√£ v√†ng)
    -> ƒë·∫£m b·∫£o m·ªói snapshot cho 1 m√£ v√†ng kh√¥ng b·ªã l·∫∑p.

    Lu·ªìng:
    - N·∫øu file ch∆∞a t·ªìn t·∫°i -> t·∫°o m·ªõi
    - N·∫øu file ƒë√£ t·ªìn t·∫°i -> ƒë·ªçc file c≈©, concat + dedup + sort r·ªìi ghi l·∫°i
    """
    os.makedirs(os.path.dirname(OUT_TABLE_NEW), exist_ok=True)

    # ƒë·∫£m b·∫£o df_new ƒë·ªß c·ªôt v√† th·ª© t·ª±
    df_new = _ensure_headers_table(df_new)

    # N·∫øu file ch∆∞a t·ªìn t·∫°i ho·∫∑c r·ªóng -> ghi lu√¥n
    if (not os.path.exists(OUT_TABLE_NEW)) or (os.path.getsize(OUT_TABLE_NEW) == 0):
        df_new.to_csv(OUT_TABLE_NEW, index=False, encoding="utf-8-sig")
        print(f"‚úÖ Created table: {OUT_TABLE_NEW} rows={len(df_new)}")
        return

    # N·∫øu file ƒë√£ c√≥ -> ƒë·ªçc ƒë·ªÉ append/dedup
    try:
        # dtype=str ƒë·ªÉ tr√°nh pandas t·ª± parse s·ªë/ng√†y l√†m thay ƒë·ªïi format
        df_old = pd.read_csv(OUT_TABLE_NEW, dtype=str, keep_default_na=False)
    except Exception as e:
        # N·∫øu file c≈© b·ªã l·ªói ƒë·ªçc -> t·∫°o l·∫°i b·∫±ng df_new
        df_new.to_csv(OUT_TABLE_NEW, index=False, encoding="utf-8-sig")
        print(f"‚ö†Ô∏è Recreated table due to read error ({e}): {OUT_TABLE_NEW} rows={len(df_new)}")
        return

    # ƒë·∫£m b·∫£o df_old c≈©ng ƒë√∫ng header
    df_old = _ensure_headers_table(df_old)

    # √©p df_new v·ªÅ string ƒë·ªÉ key gh√©p kh√¥ng b·ªã NaN/float
    df_new = df_new.astype(str)

    # concat c≈© + m·ªõi
    df_all = pd.concat([df_old, df_new], ignore_index=True)

    # Dedup key: Ng√†y | Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi | M√£ v√†ng
    key_col = "Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi"
    df_all["__key"] = (
        df_all["Ng√†y"].astype(str).str.strip() + "|" +
        df_all[key_col].astype(str).str.strip() + "|" +
        df_all["M√£ v√†ng"].astype(str).str.strip()
    )

    before = len(df_all)
    df_all = df_all.drop_duplicates(subset="__key", keep="last").drop(columns=["__key"])
    after = len(df_all)

    # sort cho ƒë·∫πp + ·ªïn ƒë·ªãnh
    df_all = df_all.sort_values(["Ng√†y", key_col, "M√£ v√†ng"])

    # ghi l·∫°i file
    df_all.to_csv(OUT_TABLE_NEW, index=False, encoding="utf-8-sig")
    print(f"‚úÖ Updated table: {OUT_TABLE_NEW} rows={after} (dedup {before}->{after})")


# Regex ƒë·ªÉ t√°ch d·∫°ng "dd/MM/yyyy HH:mm:ss {json...}" ho·∫∑c "dd/MM/yyyy HH:mm:ss{json...}"
_TS_JSON_RE = re.compile(r"^\s*(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2}:\d{2})\s*(\{.*)$")


def _split_raw_line(ts_or_line: str, maybe_json: str | None) -> tuple[str | None, str | None]:
    """
    Chu·∫©n ho√° 1 record raw th√†nh (timestamp_str, json_str)

    H·ªó tr·ª£ 2 ki·ªÉu d·ªØ li·ªáu sheet RAW_DATA:
    1) 2 c·ªôt:
       - colA: "dd/MM/yyyy HH:mm:ss"
       - colB: "{...json...}"
    2) 1 c·ªôt g·ªôp:
       - colA: "dd/MM/yyyy HH:mm:ss {...json...}"
       - ho·∫∑c "dd/MM/yyyy HH:mm:ss{...json...}"

    Tr·∫£ v·ªÅ:
      - (ts, js) n·∫øu parse ƒë∆∞·ª£c
      - (None, None) n·∫øu kh√¥ng h·ª£p l·ªá
    """
    if ts_or_line is None:
        return None, None

    a = str(ts_or_line).strip()
    b = None if maybe_json is None else str(maybe_json).strip()

    # b·ªè qua d√≤ng r·ªóng/NaN
    if a.lower() == "nan" or a == "":
        return None, None

    # Case 1: c√≥ JSON ·ªü c·ªôt B
    if b and b.lower() != "nan":
        # ƒë·∫£m b·∫£o json b·∫Øt ƒë·∫ßu t·ª´ '{'
        if "{" in b:
            b = b[b.find("{"):]
        return a, b

    # Case 2: JSON d√≠nh li·ªÅn trong c·ªôt A -> d√πng regex t√°ch
    m = _TS_JSON_RE.match(a)
    if m:
        ts = m.group(1).strip()
        js = m.group(2).strip()
        return ts, js

    # fallback: n·∫øu ch·ªâ c√≥ json m√† kh√¥ng c√≥ timestamp -> b·ªè qua (tu·ª≥ b·∫°n mu·ªën gi·ªØ hay kh√¥ng)
    if a.startswith("{") and a.endswith("}"):
        return None, a

    return None, None


def append_dedup_raw_log(df_raw: pd.DataFrame) -> None:
    """
    Append RAW_DATA v√†o OUT_RAW_LOG theo format:
      dd/MM/yyyy HH:mm:ss <json>

    Dedup theo timestamp (19 k√Ω t·ª± ƒë·∫ßu) ƒë·ªÉ tr√°nh file log ph√¨nh khi workflow ch·∫°y l·∫∑p.
    """
    os.makedirs(os.path.dirname(OUT_RAW_LOG), exist_ok=True)

    # T·∫≠p timestamp ƒë√£ t·ªìn t·∫°i trong log (ƒë·ªÉ dedup)
    existing_ts: set[str] = set()

    # N·∫øu log ƒë√£ t·ªìn t·∫°i -> ƒë·ªçc to√†n b·ªô timestamp ƒë·∫ßu d√≤ng v√†o set
    if os.path.exists(OUT_RAW_LOG) and os.path.getsize(OUT_RAW_LOG) > 0:
        try:
            with open(OUT_RAW_LOG, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    # timestamp ·ªü ƒë·∫ßu d√≤ng: "dd/MM/yyyy HH:mm:ss" => 19 k√Ω t·ª±
                    ts = line[:19].strip()
                    if ts:
                        existing_ts.add(ts)
        except Exception as e:
            # N·∫øu ƒë·ªçc file log l·ªói th√¨ v·∫´n cho pipeline ch·∫°y, ch·ªâ kh√¥ng dedup ƒë∆∞·ª£c file c≈©
            print(f"‚ö†Ô∏è Could not read existing raw log for dedup ({e}). Will append anyway.")
            existing_ts = set()

    wrote = 0
    skipped = 0

    # Append mode
    with open(OUT_RAW_LOG, "a", encoding="utf-8") as f:
        for _, row in df_raw.iterrows():
            # L·∫•y 2 c·ªôt ƒë·∫ßu c·ªßa sheet RAW_DATA (n·∫øu c√≥)
            col_a = row.iloc[0] if len(row) > 0 else None
            col_b = row.iloc[1] if len(row) > 1 else None

            ts, js = _split_raw_line(col_a, col_b)
            if not ts or not js:
                # b·ªè qua record kh√¥ng parse ƒë∆∞·ª£c
                continue

            # ƒë·∫£m b·∫£o json b·∫Øt ƒë·∫ßu t·ª´ '{'
            if "{" in js:
                js = js[js.find("{"):]
            js = js.strip()

            # dedup theo timestamp
            if ts in existing_ts:
                skipped += 1
                continue

            # ghi 1 d√≤ng snapshot
            f.write(f"{ts} {js}\n")
            existing_ts.add(ts)
            wrote += 1

    print(f"‚úÖ Raw log updated: {OUT_RAW_LOG} wrote={wrote} skipped_existing={skipped}")


def main():
    """
    Lu·ªìng t·ªïng:
    1) T·∫£i XLSX t·ª´ Google Sheets
    2) Update raw log (dedup timestamp)
    3) Update table csv (dedup snapshot+code)
    """
    print("üîÑ ƒêang t·∫£i d·ªØ li·ªáu t·ª´ Google Sheet (XLSX export)...")
    df_price, df_raw = fetch_from_sheets(INPUT_XLSX_URL)

    # 1) RAW log: timestamp + JSON (dedup by timestamp)
    append_dedup_raw_log(df_raw)

    # 2) Table: GOLD_PRICE -> CSV (dedup by Ng√†y|Th·ªùi ƒëi·ªÉm c·∫≠p nh·∫≠t gi√° m·ªõi|M√£ v√†ng)
    save_dedup_table(df_price)


if __name__ == "__main__":
    main()
